{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"bert_finetune_colab.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ksBrxCcDMx6G","colab_type":"text"},"source":["# BERT Fine-Tuned Notebook\n","## W266 Final Project\n","### Game of Thrones Text Classification\n","### T. P. Goter\n","### Fall 2019\n","\n","This notebook is used to perform the baseline, finetuned BERT supervised text classification. The original UDA process utilized a Python script wrapped in a bash shell script. This notebook was generated in order to better show and annotate the process.\n","\n","## Acknowledgement\n","Much of this code was leveraged from the open source [UDA](https://github.com/google-research/uda). It has been adapted to the Game of Thrones dataset. "]},{"cell_type":"markdown","metadata":{"id":"qdYI2s0-Mx6H","colab_type":"text"},"source":["## Import Data Libraries"]},{"cell_type":"code","metadata":{"id":"lQnyNkS0Mx6I","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import json\n","import os\n","import tensorflow as tf\n","\n","import yaml\n","import pprint\n","\n","from absl import app\n","from absl import logging\n","\n","from google.colab import drive\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EESRS289M6wZ","colab_type":"code","colab":{}},"source":["# Clean up your session\n","tf.keras.backend.clear_session()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"59hT3X10M9f9","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"ulOaFBoOM9K2","colab_type":"code","outputId":"1683636d-3404-4233-dd76-87dfb0992592","executionInfo":{"status":"error","timestamp":1570805464895,"user_tz":240,"elapsed":1375,"user":{"displayName":"Thomas Goter","photoUrl":"","userId":"00883949598941594885"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"source":["# Mounting the drive is straightforward but required authentication each time \n","# we reset the session\n","from google.colab import drive\n","drive.mount('/content/drive')\n","drive_path = '/content/drive/Computers/My iMac/nlp_finalproject/'\n","os.listdir(drive_path)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-874b6ffb384d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdrive_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/Computers/My iMac/nlp_finalproject/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/Computers/My iMac/nlp_finalproject/'"]}]},{"cell_type":"code","metadata":{"id":"xPql0jt1Ntuh","colab_type":"code","outputId":"f472675e-7502-4dc9-cc43-ad2fc2ad7917","executionInfo":{"status":"ok","timestamp":1570805482161,"user_tz":240,"elapsed":1056,"user":{"displayName":"Thomas Goter","photoUrl":"","userId":"00883949598941594885"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["!ls /content/drive/My\\ Drive\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["'Colab Notebooks'\n"," FacialKeypointDetection\n","'Final Python Project'\n","'Getting started.pdf'\n"," Healy_Andersen_Webb_lab3-peer-review.gdoc\n"," Healy_Andersen_Webb_lab3-peer-review.pdf\n"," lab3_wordcount.gdoc\n","'MIDS Program Overview - Fall 2018.pdf'\n"," NLP_FinalProject\n","'Team Process Agreement F18 th400 Group 2 - Final Project.gdoc'\n","'Team Process Agreement F18 th400 Group 2 - Project 2.gdoc'\n","'Untitled document.gdoc'\n","'Untitled spreadsheet.gsheet'\n","'w200 - 2018 Fall Calendar.gsheet'\n"," W201_G2_P1_Thoughts.gslides\n","'W201 RDADA'\n"," W207_FKD_Baseline.gslides\n"," W266_ProjectProposal_Goter.gdoc\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BwMAPIkQNKRg","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"QgiCPOzHNKd_","colab_type":"code","colab":{}},"source":["import uda\n","from bert import modeling\n","from utils import proc_data_utils\n","from utils import raw_data_utils"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oqsdumz6Mx6K","colab_type":"text"},"source":["## Define Some Options\n","This section replaces passing the input parameters as command line arguments. This section is very important. It controls the entire model. See the dictionary below.\n","\n","### Task Options:\n","- **do_train:** Boolean of whether we are training\n","- **do_eval:** Boolean of whether we are just evaluating\n","\n","### Training Options:\n","- **sup_train_data_dir:** Input directory for supervised data. This should be set to \"./Data/proc_data/train_##\" where the ## is one of the subsets of training data generated from the prepro_ALL.csh script.\n","- **eval_data_dir:**  The input data dir of the evaluation data. This should be the path to the development data with which we will do hyperparameter tuning. We can change this to the test data directory once we are ready for final evaluation. The dev data path is: \"./Data/proc_data/dev\"\n","- **unsup_data_dir:** The input data dir of the unsupervised data. Path for the unsupervised, augmented data. This should be equal to \"./Data/proc_data/unsup\"\n","- **bert_config_file:** Absolute path to the json file corresponding to the pre-trained BERT model. For us this is: \"./bert_pretrained/bert_base/bert_config.json\"\n","- **vocab_file:** The vocabulary file that the BERT model was trained on. This should be equal to \"./bert_pretrained/bert_base/vocab.txt\"\n","- **init_checkpoint:** Initial checkpoint from the pre-trained BERT model. This should be equal to: \"./bert_pretrained/bert_base/bert_model.ckpt\"\n","- **task_name:** The name of the task to train. This should be equal to \"GoT\"\n","- **model_dir:** The output directory where the model checkpoints will be written. This will be set to \"models\" followed by a case specific identifier.\n","\n","### Model configuration\n","- **use_one_hot_embeddings:** Boolean, default: True, If True, tf.one_hot will be used for embedding lookups, otherwise tf.nn.embedding_lookup will be used. On TPUs, this should be True since it is much faster.\"\n","- **max_seq_length\":** Integer, default = 128, The maximum total sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded. Note, GoT data was processed to be on-average close to this length to minimize lost data.\n","- **model_dropout:** Float, default = -1 (i.e., no dropout). Dropout rate for both the attention and the hidden states.\n","\n","### Training hyper-parameters\n","- **train_batch_size:** Integer, default = 32. Based on the discussion here https://github.com/google-research/bert#out-of-memory-issues. 32 is probably the largest we can run with 11 GB of RAM while using BERT base with a maximum sequence length of 128.\n","- **eval_batch_size:** Integer, default = 8, \"Base batch size for evaluation.\"\n","- **save_checkpoints_num:** Integer, default = 20, Number of checkpoints to save during training.\n","- **iterations_per_loop:** Integer, default = 200, Number of steps to make in each estimator call.\n","- **num_train_steps:** Integer, no default, number of training steps\n","\n","### Optimizer hyperparameters\n","- **learning_rate:** Float, default = 2e-5, The initial learning rate for Adam Optimizer\n","- **num_warmup_steps:** Integer, no default, Number of warmup steps\n","- **clip_norm:** Float, default= 1.0, Gradient clip hyperparameter.\n","\n","### UDA Options:\n","- **unsup_ratio:** Integer - ratio between unsupervised batch size and supervised batch size. If zero - dont use\n","- **aug_ops:** String - what augmentation procedure do you want to run\n","- **aug_copy:** Integer - how many augmentations per example are to be generated\n","- **uda_coeff:** Float - default 1 - This is the coefficient on the UDA loss. Basically you can rely more or less on the UDA loss during the supervised training. The UDA paper generally kept this at 1\n","- **tsa:** String - Annealing schedule to use. Options provided are \"\" none, linear_schedule, log_schedule, exp_schedule\n","- **uda_softmax_temp:** Float, default -1, A smaller temperature will accentuate differences in probabilities. Low temps were used in the UDA paper for cases with low numbers of labeled data, after masking out uncertain predictions.\n","- **uda_confidence_thresh:** Float, default -1, Threshold value above which the consistency loss term from the UDA is used. Basically ensures we are using loss from random guesses.\n","\n","### TPU and GPU Options:\n","- **use_tpu:** Boolean - self-explanatory - it affects how the model is run. If we run in colab this could be important. False means use CPU or GPU. We will default to FALSE.\n","- **tpu_name:** String - address of the tpu\n","- **gcp_project:** String - project name when using TPU\n","- **tpu_zone:** String - can be set or detected\n","- **master:** Address of the TPU master, if applicable\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wdairvWPMx6L","colab_type":"text"},"source":["### Defaults\n","\n","The defaults below should not be changed. Note that a config file will be read in after this in order to update these if desired."]},{"cell_type":"code","metadata":{"id":"Uv0zd_JKMx6L","colab_type":"code","colab":{}},"source":["options = {\n","### Training Options:\n","'bert_config_file' : \"./bert_pretrained/bert_base/bert_config.json\",\n","'vocab_file' : \"./bert_pretrained/bert_base/vocab.txt\",\n","'init_checkpoint' : \"./bert_pretrained/bert_base/bert_model.ckpt\",\n","'task_name' : \"GoT\",\n","\n","### Directory locations:\n","'sup_train_data_dir': None,\n","'eval_data_dir': None,\n","'unsup_data_dir': None,\n","    \n","### Model configuration\n","'use_one_hot_embeddings' : True,\n","'max_seq_length' : 128,\n","'model_dropout' : -1 ,\n","\n","### Training hyper-parameters\n","'train_batch_size' : 32,\n","'eval_batch_size' : 8,\n","'save_checkpoints_num' : 20,\n","'iterations_per_loop' : 200,\n","\n","### Optimizer hyperparameters\n","'learning_rate' : 2e-5,\n","'clip_norm' : 1.0,\n","\n","### UDA Options - only important if using UDA\n","'aug_ops': \"\",\n","'aug_copy': -1,\n","'unsup_ratio' : 0,\n","'uda_coeff' : 1 ,\n","'tsa' : \"\" ,\n","'uda_softmax_temp' : -1,\n","'uda_confidence_thresh' : -1,\n","\n","### TPU and GPU Options:\n","'use_tpu': False,\n","'master' : None\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nwXH9dRPMx6O","colab_type":"text"},"source":["## Set the Case to Run\n","This will ensure that different configurations are being controlled and saved separately. Just load in the correct yaml file that specifies all of the parameters."]},{"cell_type":"code","metadata":{"id":"3qALbQPhMx6P","colab_type":"code","outputId":"027f57ff-5960-4122-abdb-a4804c87c05c","colab":{}},"source":["# Set the config file to load - controls what is run\n","config = 'base_20'\n","with open('./config/' + config + '.yml', 'r') as config_in:\n","    options_from_file = yaml.safe_load(config_in)\n","    print()\n","    print(\"=\"*50 + \"\\nCase Specific Options: \\n\" + \"=\"*50)\n","    pprint.pprint(options_from_file)\n","\n","# merge dictionaries    \n","options.update(options_from_file)\n","\n","#\n","print()\n","print(\"=\"*50 + \"\\nFull Listing of Options: \\n\" + \"=\"*50)\n","pprint.pprint(options)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","==================================================\n","Case Specific Options: \n","==================================================\n","{'bert_config_file': './bert_pretrained/bert_base/bert_config.json',\n"," 'do_eval': True,\n"," 'do_train': True,\n"," 'eval_data_dir': './Data/proc_data/GoT/dev',\n"," 'init_checkpoint': './bert_pretrained/bert_base/bert_model.ckpt',\n"," 'learning_rate': '3e-05',\n"," 'model_dir': 'model/base_20',\n"," 'num_train_steps': 50,\n"," 'num_warmup_steps': 20,\n"," 'sup_train_data_dir': './Data/proc_data/GoT/train_20',\n"," 'task_name': 'GoT',\n"," 'use_tpu': False,\n"," 'vocab_file': './bert_pretrained/bert_base/vocab.txt'}\n","\n","==================================================\n","Full Listing of Options: \n","==================================================\n","{'aug_copy': -1,\n"," 'aug_ops': '',\n"," 'bert_config_file': './bert_pretrained/bert_base/bert_config.json',\n"," 'clip_norm': 1.0,\n"," 'do_eval': True,\n"," 'do_train': True,\n"," 'eval_batch_size': 8,\n"," 'eval_data_dir': './Data/proc_data/GoT/dev',\n"," 'init_checkpoint': './bert_pretrained/bert_base/bert_model.ckpt',\n"," 'iterations_per_loop': 200,\n"," 'learning_rate': '3e-05',\n"," 'master': None,\n"," 'max_seq_length': 128,\n"," 'model_dir': 'model/base_20',\n"," 'model_dropout': -1,\n"," 'num_train_steps': 50,\n"," 'num_warmup_steps': 20,\n"," 'save_checkpoints_num': 20,\n"," 'sup_train_data_dir': './Data/proc_data/GoT/train_20',\n"," 'task_name': 'GoT',\n"," 'train_batch_size': 32,\n"," 'tsa': '',\n"," 'uda_coeff': 1,\n"," 'uda_confidence_thresh': -1,\n"," 'uda_softmax_temp': -1,\n"," 'unsup_data_dir': None,\n"," 'unsup_ratio': 0,\n"," 'use_one_hot_embeddings': True,\n"," 'use_tpu': False,\n"," 'vocab_file': './bert_pretrained/bert_base/vocab.txt'}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zf8A96u7Mx6T","colab_type":"text"},"source":["## Setup the Job\n","This section of the code grabs the right data and reads in the BERT config file. We also dump our configuration options to a JSON file in the model directory."]},{"cell_type":"code","metadata":{"id":"Gc3W92qMMx6T","colab_type":"code","outputId":"e7b0f529-b80e-4bfd-e0a3-94a65c2228fc","colab":{}},"source":["# Record informational logs\n","logging.set_verbosity(logging.INFO)\n","\n","# Specify the task as that controls how the data is read and cleaned\n","processor = raw_data_utils.get_processor(options['task_name'])\n","\n","# Read in the labels\n","label_list = processor.get_labels()\n","\n","# Check the labels  -  they should be 1 through 5\n","print(label_list)\n","\n","# Read the BertConfig File\n","bert_config = modeling.BertConfig.from_json_file(\n","      options['bert_config_file'],\n","      options['model_dropout'])\n","\n","# Create the directory for the current model\n","tf.io.gfile.makedirs(options['model_dir'])\n","\n","tf.io.write_file(os.path.join(options['model_dir'], \"OPTIONS.json\"), json.dumps(options))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['1', '2', '3', '4', '5']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tf.Operation 'WriteFile' type=WriteFile>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"28-W8iYYMx6V","colab_type":"text"},"source":["## Model Specific Setup"]},{"cell_type":"code","metadata":{"id":"kD-1DKtGMx6W","colab_type":"code","outputId":"c1142a48-7918-434b-d6d2-6b5652823fb8","colab":{}},"source":["logging.info(\"warmup steps {}/{}\".format(\n","      options['num_warmup_steps'], options['num_train_steps']))\n","\n","# Specify where the checkpoints will be saved. This is just integer division between the total number of training steps and the number of checkpoints\n","save_checkpoints_steps = options['num_train_steps'] // options['save_checkpoints_num']\n","\n","# Log the checkpoints\n","logging.info(\"setting save checkpoints steps to {:d}\".format(\n","      save_checkpoints_steps))\n","\n","# Update iterations per loop\n","options['iterations_per_loop'] = min(save_checkpoints_steps,\n","                                  options['iterations_per_loop'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:absl:warmup steps 20/50\n","INFO:absl:setting save checkpoints steps to 2\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"S_YKYc5wMx6Y","colab_type":"text"},"source":["## Setup Hardware and Run Configuration"]},{"cell_type":"code","metadata":{"id":"Zps-1VwSMx6Y","colab_type":"code","outputId":"701fd3dc-b000-47b4-e16f-6b4abc649bb7","colab":{}},"source":["# If you want to run on TPUs, make sure you have the appropriate information in the config file. This will then create a ClusterResolver object with that info\n","if options['use_tpu'] and options['tpu_name']:\n","    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n","        options['tpu_name'], zone=options['tpu_zone'], project=options['gcp_project'])\n","else:\n","    tpu_cluster_resolver = None\n","\n","is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n","run_config = tf.contrib.tpu.RunConfig(\n","      cluster=tpu_cluster_resolver,\n","      master=options['master'],\n","      model_dir=options['model_dir'],\n","      save_checkpoints_steps=save_checkpoints_steps,\n","      keep_checkpoint_max=1000,\n","      tpu_config=tf.contrib.tpu.TPUConfig(\n","          iterations_per_loop=options['iterations_per_loop'],\n","          per_host_input_for_training=is_per_host))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"NJXV-fIIMx6a","colab_type":"text"},"source":["## Create our model\n","Feed our hyperparameters and model configuration information to the model function builder in the uda module"]},{"cell_type":"code","metadata":{"id":"LbcYMsPXMx6c","colab_type":"code","outputId":"b50d600b-544a-45b2-a0c0-53d64f66bee1","colab":{}},"source":["model_fn = uda.model_fn_builder(\n","      bert_config=bert_config,\n","      init_checkpoint=options['init_checkpoint'],\n","      learning_rate=options['learning_rate'],\n","      clip_norm=options['clip_norm'],\n","      num_train_steps=options['num_train_steps'],\n","      num_warmup_steps=options['num_warmup_steps'],\n","      use_tpu=options['use_tpu'],\n","      use_one_hot_embeddings=options['use_one_hot_embeddings'],\n","      num_labels=len(label_list),\n","      unsup_ratio=options['unsup_ratio'],\n","      uda_coeff=options['uda_coeff'],\n","      tsa=options['tsa'],\n","      print_feature=False,\n","      print_structure=False,\n","  )\n","\n","# If TPU is not available, this will fall back to normal Estimator on CPU or GPU.\n","estimator = tf.contrib.tpu.TPUEstimator(\n","      use_tpu=options['use_tpu'],\n","      model_fn=model_fn,\n","      config=run_config,\n","      params={\"model_dir\": options['model_dir']},\n","      train_batch_size=options['train_batch_size'],\n","      eval_batch_size=options['eval_batch_size'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'model/base_20', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 2, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 1000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77a86e6810>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=2, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': None}\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'model/base_20', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 2, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 1000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77a86e6810>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=2, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': None}\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:_TPUContext: eval_on_tpu True\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:_TPUContext: eval_on_tpu True\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"IqUg8E80Mx6e","colab_type":"text"},"source":["## Ready to Train"]},{"cell_type":"code","metadata":{"id":"x5JGFuehMx6f","colab_type":"code","outputId":"f922d9b7-6aeb-4267-af56-66de2206cd52","colab":{}},"source":["# Logical check to determine if we are training (vice evaluating)\n","if options['do_train']:\n","    logging.info(\"  >>> sup data dir : {}\".format(options['sup_train_data_dir']))\n","    \n","    # Are we doing UDA or just simple finetuning?\n","    if options['unsup_ratio'] > 0:\n","        logging.info(\"  >>> unsup data dir : {}\".format(\n","          options['unsup_data_dir']))\n","    \n","    # Pass on all of the training sup/unsup options\n","    train_input_fn = proc_data_utils.training_input_fn_builder(\n","        options['sup_train_data_dir'],\n","        options['unsup_data_dir'],\n","        options['aug_ops'],\n","        options['aug_copy'],\n","        options['unsup_ratio'],\n","        max_seq_len=options['max_seq_length'])\n","\n","# Logical check to see if we are evaluating against the development set (or test set if you change the eval_data_dir)\n","if options['do_eval']:\n","    logging.info(\"  >>> dev data dir : {}\".format(options['eval_data_dir']))\n","    eval_input_fn = proc_data_utils.evaluation_input_fn_builder(\n","        options['eval_data_dir'],\n","        \"clas\")\n","\n","    eval_size = processor.get_dev_size()\n","    eval_steps = int(eval_size / options['eval_batch_size'])\n","\n","# IF we are training and evaluating\n","if options['do_train'] and options['do_eval']:\n","    logging.info(\"***** Running training & evaluation *****\")\n","    logging.info(\"  Supervised batch size = {:d}\".format(\n","        options['train_batch_size']))\n","    logging.info(\"  Unsupervised batch size = {:d}\".format(\n","        options['train_batch_size'] * options['unsup_ratio']))\n","    logging.info(\"  Num steps = {}\".format(options['num_train_steps']))\n","    logging.info(\"  Base evaluation batch size = {:d}\".format(\n","        options['eval_batch_size']))\n","    logging.info(\"  Num steps = {:d}\".format(eval_steps))\n","    \n","    # Initialize\n","    best_acc = 0\n","    \n","    # Looping over training steps by subset (for each checkpoint)\n","    for _ in range(0, options['num_train_steps'], save_checkpoints_steps):\n","        logging.info(\"*** Running training ***\")\n","        \n","        estimator.train(\n","              input_fn=train_input_fn,\n","              steps=save_checkpoints_steps)\n","        \n","        logging.info(\"*** Running evaluation ***\")\n","        dev_result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n","        logging.info(\">> Results:\")\n","        \n","        # Keep track of the evaluation results\n","        for key in dev_result.keys():\n","            logging.info(\"  {} = {}\".format(key, str(dev_result[key])))\n","            dev_result[key] = dev_result[key].item()\n","        \n","        # Update the best accuracy object\n","        best_acc = max(best_acc, dev_result[\"eval_classify_accuracy\"])\n","    logging.info(\"***** Final evaluation result *****\")\n","    logging.info(\"Best acc: {:.3f}\\n\\n\".format(best_acc))\n","elif options['do_train']:\n","    logging.info(\"***** Running training *****\")\n","    logging.info(\"  Supervised batch size = {}\".format(options['train_batch_size']))\n","    logging.info(\"  Unsupervised batch size = {}\".format(\n","                    options['train_batch_size'] * options['unsup_ratio']))\n","    logging.info(\"  Num steps = {}\".format(options['num_train_steps']))\n","    estimator.train(input_fn=train_input_fn, max_steps=options['num_train_steps'])\n","elif options['do_eval']:\n","    logging.info(\"***** Running evaluation *****\")\n","    logging.info(\"  Base evaluation batch size = {}\".format(options['eval_batch_size']))\n","    logging.info(\"  Num steps = {}\".format(eval_steps))\n","    \n","    # Load in the checkpoint from training to do the evaluation\n","    checkpoint_state = tf.train.get_checkpoint_state(options['model_dir'])\n","\n","    best_acc = 0\n","    for ckpt_path in checkpoint_state.all_model_checkpoint_paths:\n","        if not tf.io.gfile.exists(ckpt_path + \".data-00000-of-00001\"):\n","            logging.info(\n","                \"Warning: checkpoint {:s} does not exist\".format(ckpt_path))\n","        continue\n","        logging.info(\"Evaluating {:s}\".format(ckpt_path))\n","        dev_result = estimator.evaluate(\n","          input_fn=eval_input_fn,\n","          steps=eval_steps,\n","          checkpoint_path=ckpt_path,)\n","        logging.info(\">> Results:\")\n","        \n","        # keep track of evaluation metrics\n","        for key in dev_result.keys():\n","            logging.info(\"  {:s} = {:s}\".format(key, str(dev_result[key])))\n","            dev_result[key] = dev_result[key].item()\n","        \n","        # update our best accuracy variable\n","        best_acc = max(best_acc, dev_result[\"eval_classify_accuracy\"])\n","    logging.info(\"***** Final evaluation result *****\")\n","    logging.info(\"Best acc: {:.3f}\\n\\n\".format(best_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:absl:  >>> sup data dir : ./Data/proc_data/GoT/train_20\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:looking in ./Data/proc_data/GoT/train_20 for files\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:looking in ./Data/proc_data/GoT/train_20 for files\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loading training data from these files: ./Data/proc_data/GoT/train_20/tf_examples.tfrecord.0.0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loading training data from these files: ./Data/proc_data/GoT/train_20/tf_examples.tfrecord.0.0\n","INFO:absl:  >>> dev data dir : ./Data/proc_data/GoT/dev\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loading eval clas data from these files: ./Data/proc_data/GoT/dev/tf_examples.tfrecord.0.0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loading eval clas data from these files: ./Data/proc_data/GoT/dev/tf_examples.tfrecord.0.0\n","INFO:absl:***** Running training & evaluation *****\n","INFO:absl:  Supervised batch size = 32\n","INFO:absl:  Unsupervised batch size = 0\n","INFO:absl:  Num steps = 50\n","INFO:absl:  Base evaluation batch size = 8\n","INFO:absl:  Num steps = 312\n","INFO:absl:*** Running training ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:sup batch size: 32\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:sup batch size: 32\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:***** Max Sequence Length = 128 *****\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:***** Max Sequence Length = 128 *****\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:sup batch size: 32\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:sup batch size: 32\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:total sample in a batch: 32\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:total sample in a batch: 32\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running train on CPU\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Running train on CPU\n"],"name":"stderr"},{"output_type":"stream","text":["ERROR:tensorflow:Error recorded from training_loop: name 'long' is not defined\n"],"name":"stdout"},{"output_type":"stream","text":["ERROR:tensorflow:Error recorded from training_loop: name 'long' is not defined\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:training_loop marked as finished\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:training_loop marked as finished\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Reraising captured error\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Reraising captured error\n"],"name":"stderr"},{"output_type":"error","ename":"NameError","evalue":"name 'long' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-8a802e26ffdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         estimator.train(\n\u001b[1;32m     49\u001b[0m               \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m               steps=save_checkpoints_steps)\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*** Running evaluation ***\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m   2874\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2875\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2876\u001b[0;31m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2878\u001b[0m   def evaluate(self,\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py\u001b[0m in \u001b[0;36mraise_errors\u001b[0;34m(self, timeout_sec)\u001b[0m\n\u001b[1;32m    129\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reraising captured error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkept_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m   2869\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2870\u001b[0m           \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2871\u001b[0;31m           saving_listeners=saving_listeners)\n\u001b[0m\u001b[1;32m   2872\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2873\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_loop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1186\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1188\u001b[0;31m           features, labels, ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m   1189\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   2707\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m         return super(TPUEstimator, self)._call_model_fn(features, labels, mode,\n\u001b[0;32m-> 2709\u001b[0;31m                                                         config)\n\u001b[0m\u001b[1;32m   2710\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2711\u001b[0m       return super(TPUEstimator, self)._call_model_fn(features, labels, mode,\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36m_model_fn\u001b[0;34m(features, labels, mode, config, params)\u001b[0m\n\u001b[1;32m   2965\u001b[0m           \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running %s on CPU'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2966\u001b[0m           estimator_spec = model_fn_wrapper.call_without_tpu(\n\u001b[0;32m-> 2967\u001b[0;31m               features, labels, is_export_mode=is_export_mode)\n\u001b[0m\u001b[1;32m   2968\u001b[0m           if (self._log_every_n_steps is not None\n\u001b[1;32m   2969\u001b[0m               or self._log_every_n_secs is not None):\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mcall_without_tpu\u001b[0;34m(self, features, labels, is_export_mode)\u001b[0m\n\u001b[1;32m   1547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_without_tpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_export_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1549\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_export_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_export_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_add_embedding_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_dummy_table_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, is_export_mode)\u001b[0m\n\u001b[1;32m   1865\u001b[0m       \u001b[0m_add_item_to_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_CTX_KEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m     \u001b[0mestimator_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m     if (running_on_cpu and\n\u001b[1;32m   1869\u001b[0m         isinstance(estimator_spec, model_fn_lib._TPUEstimatorSpec)):  # pylint: disable=protected-access\n","\u001b[0;32m~/nlp_finalproject/uda.py\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m    278\u001b[0m          \u001b[0munsup_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munsup_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m          \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m          \u001b[0mnum_train_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_train_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m          )\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/nlp_finalproject/uda.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(bert_config, is_training, input_ids, input_mask, input_type_ids, labels, num_labels, use_one_hot_embeddings, tsa, unsup_ratio, global_step, num_train_steps)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0minput_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       use_one_hot_embeddings=use_one_hot_embeddings)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m   clas_logits = hidden_to_logits(\n","\u001b[0;32m~/nlp_finalproject/bert/modeling.py\u001b[0m in \u001b[0;36mbert_model\u001b[0;34m(config, is_training, input_ids, input_mask, token_type_ids, input_embedding, output_type, use_one_hot_embeddings, scope)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0muse_one_hot_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         scope)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/nlp_finalproject/bert/modeling.py\u001b[0m in \u001b[0;36mbert_embedding\u001b[0;34m(config, is_training, input_ids, input_mask, token_type_ids, use_one_hot_embeddings, scope)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_probs_dropout_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m   \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_shape_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_rank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/nlp_finalproject/bert/modeling.py\u001b[0m in \u001b[0;36mget_shape_list\u001b[0;34m(tensor, expected_rank, name)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mexpected_rank\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m     \u001b[0massert_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_rank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/nlp_finalproject/bert/modeling.py\u001b[0m in \u001b[0;36massert_rank\u001b[0;34m(tensor, expected_rank, name)\u001b[0m\n\u001b[1;32m    965\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"asserting rank for {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'long' is not defined"]}]},{"cell_type":"code","metadata":{"id":"64r-dlNzMx6h","colab_type":"code","outputId":"a95af12a-57c3-4dab-d26f-61bd2b96a92e","colab":{}},"source":["options['sup_train_data_dir']"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'./Data/proc_data/GoT/train_20'"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"1emhNmanMx6j","colab_type":"code","outputId":"98de36f8-a4dc-42d4-97fb-7645b42ec3c5","colab":{}},"source":["ls ./Data/proc_data/GoT/train_20/\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf_examples.tfrecord.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9Az6sn8aMx6k","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}