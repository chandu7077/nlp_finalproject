{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Fine-Tuned Notebook\n",
    "## W266 Final Project\n",
    "### Game of Thrones Text Classification\n",
    "### T. P. Goter\n",
    "### Fall 2019\n",
    "\n",
    "This notebook is used to perform the baseline, finetuned BERT supervised text classification. The original UDA process utilized a Python script wrapped in a bash shell script. This notebook was generated in order to better show and annotate the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import uda\n",
    "from bert import modeling\n",
    "from utils import proc_data_utils\n",
    "from utils import raw_data_utils\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Some Options\n",
    "This section replaces passing the input parameters as command line arguments. This section is very important. It controls the entire model. See the dictionary below.\n",
    "\n",
    "### Task Options:\n",
    "- do_train: Boolean of whether we are training\n",
    "- do_eval: Boolean of whether we are just evaluating\n",
    "\n",
    "### Training Options:\n",
    "flags.DEFINE_string(\n",
    "    \"sup_train_data_dir\", None,\n",
    "    help=\"The input data dir of the supervised data. Should contain\"\n",
    "    \"`tf_examples.tfrecord*`\")\n",
    "flags.DEFINE_string(\n",
    "    \"eval_data_dir\", None,\n",
    "    help=\"The input data dir of the evaluation data. Should contain \"\n",
    "    \"`tf_examples.tfrecord*`\")\n",
    "flags.DEFINE_string(\n",
    "    \"unsup_data_dir\", None,\n",
    "    help=\"The input data dir of the unsupervised data. Should contain \"\n",
    "    \"`tf_examples.tfrecord*`\")\n",
    "flags.DEFINE_string(\n",
    "    \"bert_config_file\", None,\n",
    "    help=\"The config json file corresponding to the pre-trained BERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "flags.DEFINE_string(\n",
    "    \"vocab_file\", None,\n",
    "    help=\"The vocabulary file that the BERT model was trained on.\")\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\", None,\n",
    "    help=\"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "flags.DEFINE_string(\n",
    "    \"task_name\", None,\n",
    "    help=\"The name of the task to train.\")\n",
    "flags.DEFINE_string(\n",
    "    \"model_dir\", None,\n",
    "    help=\"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "### UDA Options:\n",
    "- unsup_ratio: Integer - ratio between unsupervised batch size and supervised batch size. If zero - dont use\n",
    "- aug_ops: String - what augmentation procedure do you want to run\n",
    "- aug_copy: Integer - how many augmentations per example are to be generated\n",
    "- uda_coeff: Float - default 1 - This is the coefficient on the UDA loss. Basically you can rely more or less on the UDA loss during the supervised training. The UDA paper generally kept this at 1\n",
    "- tsa: String - Annealing schedule to use. Options provided are \"\" none, linear_schedule, log_schedule, exp_schedule\n",
    "- uda_softmax_temp: Float, default -1, A smaller temperature will accentuate differences in probabilities. Low temps were used in the UDA paper for cases with low numbers of labeled data, after masking out uncertain predictions.\n",
    "- uda_confidence_thresh: Float, default -1, Threshold value above which the consistency loss term from the UDA is used. Basically ensures we are using loss from random guesses.\n",
    "\n",
    "### TPU and GPU Options:\n",
    "- use_tpu: Boolean - self-explanatory - it affects how the model is run. If we run in colab this could be important. False means use CPU or GPU. We will default to FALSE.\n",
    "- tpu_name: String - address of the tpu\n",
    "- gcp_project: String - project name when using TPU\n",
    "- tpu_zone: String - can be set or detected\n",
    "- master: Address of the TPU master, if applicable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "do_train = True\n",
    "do_eval = False\n",
    "unsup_ratio = 0\n",
    "aug_ops = \"\"\n",
    "uda_coeff = 1\n",
    "tsa = \"\"\n",
    "uda_softmax_temp = -1\n",
    "uda_confidence_thresh = -1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    --use_tpu=False \\\n",
    "  --do_train=True \\\n",
    "  --do_eval=True \\\n",
    "  --sup_train_data_dir=../Data/proc_data/GoT/${sub_set}_${sup_size} \\\n",
    "  --eval_data_dir=Data/proc_data/GoT/dev \\\n",
    "  --bert_config_file=${bert_config} \\\n",
    "  --vocab_file=${bert_vocab_file} \\\n",
    "  --init_checkpoint=${bert_ckpt} \\\n",
    "  --task_name=GoT \\\n",
    "  --model_dir=ckpt/base \\\n",
    "  --num_train_steps=3000 \\\n",
    "  --learning_rate=3e-05 \\\n",
    "  --num_warmup_steps=300 \\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
