{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Fine-Tuned Notebook\n",
    "## W266 Final Project\n",
    "### Game of Thrones Text Classification\n",
    "### T. P. Goter\n",
    "### Fall 2019\n",
    "\n",
    "This notebook is used to perform the baseline, finetuned BERT supervised text classification. The original UDA process utilized a Python script wrapped in a bash shell script. This notebook was generated in order to better show and annotate the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import uda\n",
    "from bert import modeling\n",
    "from utils import proc_data_utils\n",
    "from utils import raw_data_utils\n",
    "\n",
    "import yaml\n",
    "import pprint\n",
    "\n",
    "from absl import app\n",
    "from absl import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Some Options\n",
    "This section replaces passing the input parameters as command line arguments. This section is very important. It controls the entire model. See the dictionary below.\n",
    "\n",
    "### Task Options:\n",
    "- **do_train:** Boolean of whether we are training\n",
    "- **do_eval:** Boolean of whether we are just evaluating\n",
    "\n",
    "### Training Options:\n",
    "- **sup_train_data_dir:** Input directory for supervised data. This should be set to \"./Data/proc_data/train_##\" where the ## is one of the subsets of training data generated from the prepro_ALL.csh script.\n",
    "- **eval_data_dir:**  The input data dir of the evaluation data. This should be the path to the development data with which we will do hyperparameter tuning. We can change this to the test data directory once we are ready for final evaluation. The dev data path is: \"./Data/proc_data/dev\"\n",
    "- **unsup_data_dir:** The input data dir of the unsupervised data. Path for the unsupervised, augmented data. This should be equal to \"./Data/proc_data/unsup\"\n",
    "- **bert_config_file:** Absolute path to the json file corresponding to the pre-trained BERT model. For us this is: \"./bert_pretrained/bert_base/bert_config.json\"\n",
    "- **vocab_file:** The vocabulary file that the BERT model was trained on. This should be equal to \"./bert_pretrained/bert_base/vocab.txt\"\n",
    "- **init_checkpoint:** Initial checkpoint from the pre-trained BERT model. This should be equal to: \"./bert_pretrained/bert_base/bert_model.ckpt\"\n",
    "- **task_name:** The name of the task to train. This should be equal to \"GoT\"\n",
    "- **model_dir:** The output directory where the model checkpoints will be written. This will be set to \"models\" followed by a case specific identifier.\n",
    "\n",
    "### Model configuration\n",
    "- **use_one_hot_embeddings:** Boolean, default: True, If True, tf.one_hot will be used for embedding lookups, otherwise tf.nn.embedding_lookup will be used. On TPUs, this should be True since it is much faster.\"\n",
    "- **max_seq_length\":** Integer, default = 128, The maximum total sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded. Note, GoT data was processed to be on-average close to this length to minimize lost data.\n",
    "- **model_dropout:** Float, default = -1 (i.e., no dropout). Dropout rate for both the attention and the hidden states.\n",
    "\n",
    "### Training hyper-parameters\n",
    "- **train_batch_size:** Integer, default = 32. Based on the discussion here https://github.com/google-research/bert#out-of-memory-issues. 32 is probably the largest we can run with 11 GB of RAM while using BERT base with a maximum sequence length of 128.\n",
    "- **eval_batch_size:** Integer, default = 8, \"Base batch size for evaluation.\"\n",
    "- **save_checkpoints_num:** Integer, default = 20, Number of checkpoints to save during training.\n",
    "- **iterations_per_loop:** Integer, default = 200, Number of steps to make in each estimator call.\n",
    "- **num_train_steps:** Integer, no default, number of training steps\n",
    "\n",
    "### Optimizer hyperparameters\n",
    "- **learning_rate:** Float, default = 2e-5, The initial learning rate for Adam Optimizer\n",
    "- **num_warmup_steps:** Integer, no default, Number of warmup steps\n",
    "- **clip_norm:** Float, default= 1.0, Gradient clip hyperparameter.\n",
    "\n",
    "### UDA Options:\n",
    "- **unsup_ratio:** Integer - ratio between unsupervised batch size and supervised batch size. If zero - dont use\n",
    "- **aug_ops:** String - what augmentation procedure do you want to run\n",
    "- **aug_copy:** Integer - how many augmentations per example are to be generated\n",
    "- **uda_coeff:** Float - default 1 - This is the coefficient on the UDA loss. Basically you can rely more or less on the UDA loss during the supervised training. The UDA paper generally kept this at 1\n",
    "- **tsa:** String - Annealing schedule to use. Options provided are \"\" none, linear_schedule, log_schedule, exp_schedule\n",
    "- **uda_softmax_temp:** Float, default -1, A smaller temperature will accentuate differences in probabilities. Low temps were used in the UDA paper for cases with low numbers of labeled data, after masking out uncertain predictions.\n",
    "- **uda_confidence_thresh:** Float, default -1, Threshold value above which the consistency loss term from the UDA is used. Basically ensures we are using loss from random guesses.\n",
    "\n",
    "### TPU and GPU Options:\n",
    "- **use_tpu:** Boolean - self-explanatory - it affects how the model is run. If we run in colab this could be important. False means use CPU or GPU. We will default to FALSE.\n",
    "- **tpu_name:** String - address of the tpu\n",
    "- **gcp_project:** String - project name when using TPU\n",
    "- **tpu_zone:** String - can be set or detected\n",
    "- **master:** Address of the TPU master, if applicable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defaults\n",
    "\n",
    "The defaults below should not be changed. Note that a config file will be read in after this in order to update these if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "### Training Options:\n",
    "'bert_config_file' : \"./bert_pretrained/bert_base/bert_config.json\",\n",
    "'vocab_file' : \"./bert_pretrained/bert_base/vocab.txt\",\n",
    "'init_checkpoint' : \"./bert_pretrained/bert_base/bert_model.ckpt\",\n",
    "'task_name' : \"GoT\",\n",
    "\n",
    "### Model configuration\n",
    "'use_one_hot_embeddings' : True,\n",
    "'max_seq_length' : 128,\n",
    "'model_dropout' : -1 ,\n",
    "\n",
    "### Training hyper-parameters\n",
    "'train_batch_size' : 32,\n",
    "'eval_batch_size' : 8,\n",
    "'save_checkpoints_num' : 20,\n",
    "'iterations_per_loop' : 200,\n",
    "\n",
    "### Optimizer hyperparameters\n",
    "'learning_rate' : 2e-5,\n",
    "'clip_norm' : 1.0,\n",
    "\n",
    "### UDA Options - only important if using UDA\n",
    "'uda_coeff' : 1 ,\n",
    "'tsa' : \"\" ,\n",
    "'uda_softmax_temp' : -1,\n",
    "'uda_confidence_thresh' : -1,\n",
    "\n",
    "### TPU and GPU Options:\n",
    "'use_tpu': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Case to Run\n",
    "This will ensure that different configurations are being controlled and saved separately. Just load in the correct yaml file that specifies all of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Case Specific Options: \n",
      "==================================================\n",
      "{'bert_config_file': './bert_pretrained/bert_base/bert_config.json',\n",
      " 'do_eval': True,\n",
      " 'do_train': True,\n",
      " 'eval_data_dir': 'Data/proc_data/GoT/dev',\n",
      " 'init_checkpoint': './bert_pretrained/bert_base/bert_model.ckpt',\n",
      " 'learning_rate': '3e-05',\n",
      " 'model_dir': 'model/base_20',\n",
      " 'num_train_steps': 50,\n",
      " 'num_warmup_steps': 20,\n",
      " 'sup_train_data_dir': '/Data/proc_data/GoT/train_20',\n",
      " 'task_name': 'GoT',\n",
      " 'use_tpu': False,\n",
      " 'vocab_file': './bert_pretrained/bert_base/vocab.txt'}\n",
      "\n",
      "==================================================\n",
      "Full Listing of Options: \n",
      "==================================================\n",
      "{'bert_config_file': './bert_pretrained/bert_base/bert_config.json',\n",
      " 'clip_norm': 1.0,\n",
      " 'do_eval': True,\n",
      " 'do_train': True,\n",
      " 'eval_batch_size': 8,\n",
      " 'eval_data_dir': 'Data/proc_data/GoT/dev',\n",
      " 'init_checkpoint': './bert_pretrained/bert_base/bert_model.ckpt',\n",
      " 'iterations_per_loop': 200,\n",
      " 'learning_rate': '3e-05',\n",
      " 'max_seq_length': 128,\n",
      " 'model_dir': 'model/base_20',\n",
      " 'model_dropout': -1,\n",
      " 'num_train_steps': 50,\n",
      " 'num_warmup_steps': 20,\n",
      " 'save_checkpoints_num': 20,\n",
      " 'sup_train_data_dir': '/Data/proc_data/GoT/train_20',\n",
      " 'task_name': 'GoT',\n",
      " 'train_batch_size': 32,\n",
      " 'tsa': '',\n",
      " 'uda_coeff': 1,\n",
      " 'uda_confidence_thresh': -1,\n",
      " 'uda_softmax_temp': -1,\n",
      " 'use_one_hot_embeddings': True,\n",
      " 'use_tpu': False,\n",
      " 'vocab_file': './bert_pretrained/bert_base/vocab.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Set the config file to load - controls what is run\n",
    "config = 'base_20'\n",
    "with open('./config/' + config + '.yml', 'r') as config_in:\n",
    "    options_from_file = yaml.safe_load(config_in)\n",
    "    print()\n",
    "    print(\"=\"*50 + \"\\nCase Specific Options: \\n\" + \"=\"*50)\n",
    "    pprint.pprint(options_from_file)\n",
    "\n",
    "# merge dictionaries    \n",
    "options.update(options_from_file)\n",
    "\n",
    "#\n",
    "print()\n",
    "print(\"=\"*50 + \"\\nFull Listing of Options: \\n\" + \"=\"*50)\n",
    "pprint.pprint(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'gfile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-30fa72fde961>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m bert_config = modeling.BertConfig.from_json_file(\n\u001b[1;32m     15\u001b[0m       \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bert_config_file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       options['model_dropout'])\n\u001b[0m",
      "\u001b[0;32m~/Desktop/MIDS_TPG/W266/nlp_finalproject/bert/modeling.py\u001b[0m in \u001b[0;36mfrom_json_file\u001b[0;34m(cls, json_file, model_dropout)\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m       \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'gfile'"
     ]
    }
   ],
   "source": [
    "# Record informational logs\n",
    "logging.set_verbosity(logging.INFO)\n",
    "\n",
    "# Specify the task as that controls how the data is read and cleaned\n",
    "processor = raw_data_utils.get_processor(options['task_name'])\n",
    "\n",
    "# Read in the labels\n",
    "label_list = processor.get_labels()\n",
    "\n",
    "# Check the labels  -  they should be 1 through 5\n",
    "print(label_list)\n",
    "\n",
    "# Read the BertConfig File\n",
    "bert_config = modeling.BertConfig.from_json_file(\n",
    "      options['bert_config_file'],\n",
    "      options['model_dropout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-28-aa809895a58a>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-aa809895a58a>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    tf.io.gfile.makedirs(FLAGS.model_dir)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "  tf.io.gfile.makedirs(FLAGS.model_dir)\n",
    "\n",
    "  flags_dict = app.flags.FLAGS.flag_values_dict()\n",
    "  with tf.io.read_file(os.path.join(FLAGS.model_dir, \"FLAGS.json\"), \"w\") as ouf:\n",
    "    json.dump(flags_dict, ouf)\n",
    "\n",
    "  logging.info(\"warmup steps {}/{}\".format(\n",
    "      FLAGS.num_warmup_steps, FLAGS.num_train_steps))\n",
    "\n",
    "  save_checkpoints_steps = FLAGS.num_train_steps // FLAGS.save_checkpoints_num\n",
    "  logging.info(\"setting save checkpoints steps to {:d}\".format(\n",
    "      save_checkpoints_steps))\n",
    "\n",
    "  FLAGS.iterations_per_loop = min(save_checkpoints_steps,\n",
    "                                  FLAGS.iterations_per_loop)\n",
    "  if FLAGS.use_tpu and FLAGS.tpu_name:\n",
    "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
    "  else:\n",
    "    tpu_cluster_resolver = None\n",
    "  # if not FLAGS.use_tpu and FLAGS.num_gpu > 1:\n",
    "  #   train_distribute = tf.contrib.distribute.MirroredStrategy(\n",
    "  #       num_gpus=FLAGS.num_gpu)\n",
    "  # else:\n",
    "  #   train_distribute = None\n",
    "\n",
    "  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "  run_config = tf.contrib.tpu.RunConfig(\n",
    "      cluster=tpu_cluster_resolver,\n",
    "      master=FLAGS.master,\n",
    "      model_dir=FLAGS.model_dir,\n",
    "      save_checkpoints_steps=save_checkpoints_steps,\n",
    "      keep_checkpoint_max=1000,\n",
    "      # train_distribute=train_distribute,\n",
    "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "          iterations_per_loop=FLAGS.iterations_per_loop,\n",
    "          per_host_input_for_training=is_per_host))\n",
    "\n",
    "  model_fn = uda.model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      init_checkpoint=FLAGS.init_checkpoint,\n",
    "      learning_rate=FLAGS.learning_rate,\n",
    "      clip_norm=FLAGS.clip_norm,\n",
    "      num_train_steps=FLAGS.num_train_steps,\n",
    "      num_warmup_steps=FLAGS.num_warmup_steps,\n",
    "      use_tpu=FLAGS.use_tpu,\n",
    "      use_one_hot_embeddings=FLAGS.use_one_hot_embeddings,\n",
    "      num_labels=len(label_list),\n",
    "      unsup_ratio=FLAGS.unsup_ratio,\n",
    "      uda_coeff=FLAGS.uda_coeff,\n",
    "      tsa=FLAGS.tsa,\n",
    "      print_feature=False,\n",
    "      print_structure=False,\n",
    "  )\n",
    "\n",
    "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "  # or GPU.\n",
    "  estimator = tf.contrib.tpu.TPUEstimator(\n",
    "      use_tpu=FLAGS.use_tpu,\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      params={\"model_dir\": FLAGS.model_dir},\n",
    "      train_batch_size=FLAGS.train_batch_size,\n",
    "      eval_batch_size=FLAGS.eval_batch_size)\n",
    "\n",
    "  if FLAGS.do_train:\n",
    "    logging.info(\"  >>> sup data dir : {}\".format(FLAGS.sup_train_data_dir))\n",
    "    if FLAGS.unsup_ratio > 0:\n",
    "      logging.info(\"  >>> unsup data dir : {}\".format(\n",
    "          FLAGS.unsup_data_dir))\n",
    "\n",
    "    train_input_fn = proc_data_utils.training_input_fn_builder(\n",
    "        FLAGS.sup_train_data_dir,\n",
    "        FLAGS.unsup_data_dir,\n",
    "        FLAGS.aug_ops,\n",
    "        FLAGS.aug_copy,\n",
    "        FLAGS.unsup_ratio)\n",
    "\n",
    "  if FLAGS.do_eval:\n",
    "    logging.info(\"  >>> dev data dir : {}\".format(FLAGS.eval_data_dir))\n",
    "    eval_input_fn = proc_data_utils.evaluation_input_fn_builder(\n",
    "        FLAGS.eval_data_dir,\n",
    "        \"clas\")\n",
    "\n",
    "    eval_size = processor.get_dev_size()\n",
    "    eval_steps = int(eval_size / FLAGS.eval_batch_size)\n",
    "\n",
    "  if FLAGS.do_train and FLAGS.do_eval:\n",
    "    logging.info(\"***** Running training & evaluation *****\")\n",
    "    logging.info(\"  Supervised batch size = %d\", FLAGS.train_batch_size)\n",
    "    logging.info(\"  Unsupervised batch size = %d\",\n",
    "                    FLAGS.train_batch_size * FLAGS.unsup_ratio)\n",
    "    logging.info(\"  Num steps = %d\", FLAGS.num_train_steps)\n",
    "    logging.info(\"  Base evaluation batch size = %d\", FLAGS.eval_batch_size)\n",
    "    logging.info(\"  Num steps = %d\", eval_steps)\n",
    "    best_acc = 0\n",
    "    for _ in range(0, FLAGS.num_train_steps, save_checkpoints_steps):\n",
    "      logging.info(\"*** Running training ***\")\n",
    "      estimator.train(\n",
    "          input_fn=train_input_fn,\n",
    "          steps=save_checkpoints_steps)\n",
    "      logging.info(\"*** Running evaluation ***\")\n",
    "      dev_result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
    "      logging.info(\">> Results:\")\n",
    "      for key in dev_result.keys():\n",
    "        logging.info(\"  %s = %s\", key, str(dev_result[key]))\n",
    "        dev_result[key] = dev_result[key].item()\n",
    "      best_acc = max(best_acc, dev_result[\"eval_classify_accuracy\"])\n",
    "    logging.info(\"***** Final evaluation result *****\")\n",
    "    logging.info(\"Best acc: {:.3f}\\n\\n\".format(best_acc))\n",
    "  elif FLAGS.do_train:\n",
    "    logging.info(\"***** Running training *****\")\n",
    "    logging.info(\"  Supervised batch size = %d\", FLAGS.train_batch_size)\n",
    "    logging.info(\"  Unsupervised batch size = %d\",\n",
    "                    FLAGS.train_batch_size * FLAGS.unsup_ratio)\n",
    "    logging.info(\"  Num steps = %d\", FLAGS.num_train_steps)\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)\n",
    "  elif FLAGS.do_eval:\n",
    "    logging.info(\"***** Running evaluation *****\")\n",
    "    logging.info(\"  Base evaluation batch size = %d\", FLAGS.eval_batch_size)\n",
    "    logging.info(\"  Num steps = %d\", eval_steps)\n",
    "    checkpoint_state = tf.train.get_checkpoint_state(FLAGS.model_dir)\n",
    "\n",
    "    best_acc = 0\n",
    "    for ckpt_path in checkpoint_state.all_model_checkpoint_paths:\n",
    "      if not tf.io.gfile.exists(ckpt_path + \".data-00000-of-00001\"):\n",
    "        logging.info(\n",
    "            \"Warning: checkpoint {:s} does not exist\".format(ckpt_path))\n",
    "        continue\n",
    "      logging.info(\"Evaluating {:s}\".format(ckpt_path))\n",
    "      dev_result = estimator.evaluate(\n",
    "          input_fn=eval_input_fn,\n",
    "          steps=eval_steps,\n",
    "          checkpoint_path=ckpt_path,\n",
    "      )\n",
    "      logging.info(\">> Results:\")\n",
    "      for key in dev_result.keys():\n",
    "        logging.info(\"  %s = %s\", key, str(dev_result[key]))\n",
    "        dev_result[key] = dev_result[key].item()\n",
    "      best_acc = max(best_acc, dev_result[\"eval_classify_accuracy\"])\n",
    "    logging.info(\"***** Final evaluation result *****\")\n",
    "    logging.info(\"Best acc: {:.3f}\\n\\n\".format(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
