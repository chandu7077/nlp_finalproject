{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Fine-Tuned Notebook\n",
    "## W266 Final Project\n",
    "### Game of Thrones Text Classification\n",
    "### T. P. Goter\n",
    "### Fall 2019\n",
    "\n",
    "This notebook is used to perform the baseline, finetuned BERT supervised text classification. The original UDA process utilized a Python script wrapped in a bash shell script. This notebook was generated in order to better show and annotate the process.\n",
    "\n",
    "## Acknowledgement\n",
    "Much of this code was leveraged from the open source [UDA](https://github.com/google-research/uda). It has been adapted to the Game of Thrones dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/bert/optimization.py:28: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "1.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import uda\n",
    "from bert import modeling\n",
    "from utils import proc_data_utils\n",
    "from utils import raw_data_utils\n",
    "\n",
    "import yaml\n",
    "import pprint\n",
    "\n",
    "from absl import app\n",
    "from absl import logging\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Some Options\n",
    "This section replaces passing the input parameters as command line arguments. This section is very important. It controls the entire model. See the dictionary below.\n",
    "\n",
    "### Task Options:\n",
    "- **do_train:** Boolean of whether we are training\n",
    "- **do_eval:** Boolean of whether we are just evaluating\n",
    "\n",
    "### Training Options:\n",
    "- **sup_train_data_dir:** Input directory for supervised data. This should be set to \"./Data/proc_data/train_##\" where the ## is one of the subsets of training data generated from the prepro_ALL.csh script.\n",
    "- **eval_data_dir:**  The input data dir of the evaluation data. This should be the path to the development data with which we will do hyperparameter tuning. We can change this to the test data directory once we are ready for final evaluation. The dev data path is: \"./Data/proc_data/dev\"\n",
    "- **unsup_data_dir:** The input data dir of the unsupervised data. Path for the unsupervised, augmented data. This should be equal to \"./Data/proc_data/unsup\"\n",
    "- **bert_config_file:** Absolute path to the json file corresponding to the pre-trained BERT model. For us this is: \"./bert_pretrained/bert_base/bert_config.json\"\n",
    "- **vocab_file:** The vocabulary file that the BERT model was trained on. This should be equal to \"./bert_pretrained/bert_base/vocab.txt\"\n",
    "- **init_checkpoint:** Initial checkpoint from the pre-trained BERT model. This should be equal to: \"./bert_pretrained/bert_base/bert_model.ckpt\"\n",
    "- **task_name:** The name of the task to train. This should be equal to \"GoT\"\n",
    "- **model_dir:** The output directory where the model checkpoints will be written. This will be set to \"models\" followed by a case specific identifier.\n",
    "\n",
    "### Model configuration\n",
    "- **use_one_hot_embeddings:** Boolean, default: True, If True, tf.one_hot will be used for embedding lookups, otherwise tf.nn.embedding_lookup will be used. On TPUs, this should be True since it is much faster.\"\n",
    "- **max_seq_length\":** Integer, default = 128, The maximum total sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded. Note, GoT data was processed to be on-average close to this length to minimize lost data.\n",
    "- **model_dropout:** Float, default = -1 (i.e., no dropout). Dropout rate for both the attention and the hidden states.\n",
    "\n",
    "### Training hyper-parameters\n",
    "- **train_batch_size:** Integer, default = 32. Based on the discussion here https://github.com/google-research/bert#out-of-memory-issues. 32 is probably the largest we can run with 11 GB of RAM while using BERT base with a maximum sequence length of 128.\n",
    "- **eval_batch_size:** Integer, default = 8, \"Base batch size for evaluation.\"\n",
    "- **save_checkpoints_num:** Integer, default = 20, Number of checkpoints to save during training.\n",
    "- **iterations_per_loop:** Integer, default = 200, Number of steps to make in each estimator call.\n",
    "- **num_train_steps:** Integer, no default, number of training steps\n",
    "\n",
    "### Optimizer hyperparameters\n",
    "- **learning_rate:** Float, default = 2e-5, The initial learning rate for Adam Optimizer\n",
    "- **num_warmup_steps:** Integer, no default, Number of warmup steps\n",
    "- **clip_norm:** Float, default= 1.0, Gradient clip hyperparameter.\n",
    "\n",
    "### UDA Options:\n",
    "- **unsup_ratio:** Integer - ratio between unsupervised batch size and supervised batch size. If zero - dont use\n",
    "- **aug_ops:** String - what augmentation procedure do you want to run\n",
    "- **aug_copy:** Integer - how many augmentations per example are to be generated\n",
    "- **uda_coeff:** Float - default 1 - This is the coefficient on the UDA loss. Basically you can rely more or less on the UDA loss during the supervised training. The UDA paper generally kept this at 1\n",
    "- **tsa:** String - Annealing schedule to use. Options provided are \"\" none, linear_schedule, log_schedule, exp_schedule\n",
    "- **uda_softmax_temp:** Float, default -1, A smaller temperature will accentuate differences in probabilities. Low temps were used in the UDA paper for cases with low numbers of labeled data, after masking out uncertain predictions.\n",
    "- **uda_confidence_thresh:** Float, default -1, Threshold value above which the consistency loss term from the UDA is used. Basically ensures we are using loss from random guesses.\n",
    "\n",
    "### TPU and GPU Options:\n",
    "- **use_tpu:** Boolean - self-explanatory - it affects how the model is run. If we run in colab this could be important. False means use CPU or GPU. We will default to FALSE.\n",
    "- **tpu_name:** String - address of the tpu\n",
    "- **gcp_project:** String - project name when using TPU\n",
    "- **tpu_zone:** String - can be set or detected\n",
    "- **master:** Address of the TPU master, if applicable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defaults\n",
    "\n",
    "The defaults below should not be changed. Note that a config file will be read in after this in order to update these if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "### Training Options:\n",
    "'bert_config_file' : \"./bert_pretrained/bert_base/bert_config.json\",\n",
    "'vocab_file' : \"./bert_pretrained/bert_base/vocab.txt\",\n",
    "'init_checkpoint' : \"./bert_pretrained/bert_base/bert_model.ckpt\",\n",
    "'task_name' : \"GoT\",\n",
    "\n",
    "### Directory locations:\n",
    "'sup_train_data_dir': None,\n",
    "'eval_data_dir': None,\n",
    "'unsup_data_dir': None,\n",
    "    \n",
    "### Model configuration\n",
    "'use_one_hot_embeddings' : True,\n",
    "'max_seq_length' : 128,\n",
    "'model_dropout' : -1 ,\n",
    "\n",
    "### Training hyper-parameters\n",
    "'train_batch_size' : 8,\n",
    "'eval_batch_size' : 8,\n",
    "'save_checkpoints_num' : 20,\n",
    "'iterations_per_loop' : 200,\n",
    "\n",
    "### Optimizer hyperparameters\n",
    "'learning_rate' : 2e-5,\n",
    "'clip_norm' : 1.0,\n",
    "\n",
    "### UDA Options - only important if using UDA\n",
    "'aug_ops': \"\",\n",
    "'aug_copy': -1,\n",
    "'unsup_ratio' : 0,\n",
    "'uda_coeff' : 1 ,\n",
    "'tsa' : \"\" ,\n",
    "'uda_softmax_temp' : -1,\n",
    "'uda_confidence_thresh' : -1,\n",
    "\n",
    "### TPU and GPU Options:\n",
    "'use_tpu': False,\n",
    "'master' : None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Case to Run\n",
    "This will ensure that different configurations are being controlled and saved separately. Just load in the correct yaml file that specifies all of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Case Specific Options: \n",
      "==================================================\n",
      "{'bert_config_file': './bert_pretrained/bert_base/bert_config.json',\n",
      " 'do_eval': True,\n",
      " 'do_train': True,\n",
      " 'eval_data_dir': './Data/proc_data/GoT/dev',\n",
      " 'init_checkpoint': './bert_pretrained/bert_base/bert_model.ckpt',\n",
      " 'learning_rate': 3e-05,\n",
      " 'model_dir': 'model/base_20',\n",
      " 'num_train_steps': 3000,\n",
      " 'num_warmup_steps': 300,\n",
      " 'save_checkpoints_num': 48,\n",
      " 'sup_train_data_dir': './Data/proc_data/GoT/train_20',\n",
      " 'task_name': 'GoT',\n",
      " 'use_tpu': False,\n",
      " 'vocab_file': './bert_pretrained/bert_base/vocab.txt'}\n",
      "\n",
      "==================================================\n",
      "Full Listing of Options: \n",
      "==================================================\n",
      "{'aug_copy': -1,\n",
      " 'aug_ops': '',\n",
      " 'bert_config_file': './bert_pretrained/bert_base/bert_config.json',\n",
      " 'clip_norm': 1.0,\n",
      " 'do_eval': True,\n",
      " 'do_train': True,\n",
      " 'eval_batch_size': 8,\n",
      " 'eval_data_dir': './Data/proc_data/GoT/dev',\n",
      " 'init_checkpoint': './bert_pretrained/bert_base/bert_model.ckpt',\n",
      " 'iterations_per_loop': 200,\n",
      " 'learning_rate': 3e-05,\n",
      " 'master': None,\n",
      " 'max_seq_length': 128,\n",
      " 'model_dir': 'model/base_20',\n",
      " 'model_dropout': -1,\n",
      " 'num_train_steps': 3000,\n",
      " 'num_warmup_steps': 300,\n",
      " 'save_checkpoints_num': 48,\n",
      " 'sup_train_data_dir': './Data/proc_data/GoT/train_20',\n",
      " 'task_name': 'GoT',\n",
      " 'train_batch_size': 8,\n",
      " 'tsa': '',\n",
      " 'uda_coeff': 1,\n",
      " 'uda_confidence_thresh': -1,\n",
      " 'uda_softmax_temp': -1,\n",
      " 'unsup_data_dir': None,\n",
      " 'unsup_ratio': 0,\n",
      " 'use_one_hot_embeddings': True,\n",
      " 'use_tpu': False,\n",
      " 'vocab_file': './bert_pretrained/bert_base/vocab.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Set the config file to load - controls what is run\n",
    "config = 'base_20'\n",
    "with open('./config/' + config + '.yml', 'r') as config_in:\n",
    "    options_from_file = yaml.safe_load(config_in)\n",
    "    print()\n",
    "    print(\"=\"*50 + \"\\nCase Specific Options: \\n\" + \"=\"*50)\n",
    "    pprint.pprint(options_from_file)\n",
    "\n",
    "# merge dictionaries    \n",
    "options.update(options_from_file)\n",
    "\n",
    "#\n",
    "print()\n",
    "print(\"=\"*50 + \"\\nFull Listing of Options: \\n\" + \"=\"*50)\n",
    "pprint.pprint(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Job\n",
    "This section of the code grabs the right data and reads in the BERT config file. We also dump our configuration options to a JSON file in the model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5']\n",
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/bert/modeling.py:94: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:Setting up BERT Config using data from ./bert_pretrained/bert_base/bert_config.json\n",
      "INFO:tensorflow:Setting up BERT Config using data from {'attention_probs_dropout_prob': 0.1, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'intermediate_size': 3072, 'max_position_embeddings': 512, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'type_vocab_size': 2, 'vocab_size': 30522}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'WriteFile' type=WriteFile>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Record informational logs\n",
    "logging.set_verbosity(logging.INFO)\n",
    "\n",
    "# Specify the task as that controls how the data is read and cleaned\n",
    "processor = raw_data_utils.get_processor(options['task_name'])\n",
    "\n",
    "# Read in the labels\n",
    "label_list = processor.get_labels()\n",
    "\n",
    "# Check the labels  -  they should be 1 through 5\n",
    "print(label_list)\n",
    "\n",
    "# Read the BertConfig File\n",
    "bert_config = modeling.BertConfig.from_json_file(\n",
    "      options['bert_config_file'],\n",
    "      options['model_dropout'])\n",
    "\n",
    "# Create the directory for the current model\n",
    "tf.io.gfile.makedirs(options['model_dir'])\n",
    "\n",
    "# Dump the configuration dictionary to an output json file in the model specific directory\n",
    "tf.io.write_file(os.path.join(options['model_dir'], \"OPTIONS.json\"), json.dumps(options))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specific Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:warmup steps 300/3000\n",
      "INFO:absl:setting save checkpoints steps to 62\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"warmup steps {}/{}\".format(\n",
    "      options['num_warmup_steps'], options['num_train_steps']))\n",
    "\n",
    "# Specify where the checkpoints will be saved. This is just integer division between the total number of training steps and the number of checkpoints\n",
    "save_checkpoints_steps = options['num_train_steps'] // options['save_checkpoints_num']\n",
    "\n",
    "# Log the checkpoints\n",
    "logging.info(\"setting save checkpoints steps to {:d}\".format(\n",
    "      save_checkpoints_steps))\n",
    "\n",
    "# Update iterations per loop\n",
    "options['iterations_per_loop'] = min(save_checkpoints_steps,\n",
    "                                  options['iterations_per_loop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Hardware and Run Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If you want to run on TPUs, make sure you have the appropriate information in the config file. This will then create a ClusterResolver object with that info\n",
    "if options['use_tpu'] and options['tpu_name']:\n",
    "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "        options['tpu_name'], zone=options['tpu_zone'], project=options['gcp_project'])\n",
    "else:\n",
    "    tpu_cluster_resolver = None\n",
    "\n",
    "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "run_config = tf.estimator.tpu.RunConfig(\n",
    "      cluster=tpu_cluster_resolver,\n",
    "      master=options['master'],\n",
    "     model_dir=options['model_dir'],\n",
    "#      save_checkpoints_steps=save_checkpoints_steps,\n",
    "#      keep_checkpoint_max=1000,\n",
    "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "          iterations_per_loop=options['iterations_per_loop'],\n",
    "          per_host_input_for_training=is_per_host))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our model\n",
    "Feed our hyperparameters and model configuration information to the model function builder in the uda module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'model/base_20', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f24d931e250>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=62, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'model/base_20', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f24d931e250>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=62, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n"
     ]
    }
   ],
   "source": [
    "model_fn = uda.model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      init_checkpoint=options['init_checkpoint'],\n",
    "      learning_rate=options['learning_rate'],\n",
    "      clip_norm=options['clip_norm'],\n",
    "      num_train_steps=options['num_train_steps'],\n",
    "      num_warmup_steps=options['num_warmup_steps'],\n",
    "      use_tpu=options['use_tpu'],\n",
    "      use_one_hot_embeddings=options['use_one_hot_embeddings'],\n",
    "      num_labels=len(label_list),\n",
    "      unsup_ratio=options['unsup_ratio'],\n",
    "      uda_coeff=options['uda_coeff'],\n",
    "      tsa=options['tsa'],\n",
    "      print_feature=False,\n",
    "      print_structure=False,\n",
    "  )\n",
    "\n",
    "# # If TPU is not available, this will fall back to normal Estimator on CPU or GPU.\n",
    "estimator = tf.estimator.tpu.TPUEstimator(\n",
    "      use_tpu=options['use_tpu'],\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "     params={\"model_dir\": options['model_dir']},\n",
    "      train_batch_size=options['train_batch_size'],\n",
    "      eval_batch_size=options['eval_batch_size'])\n",
    "\n",
    "# # Use base Estimator Class instead of tpu derivative\n",
    "# estimator = tf.estimator.Estimator(\n",
    "#       model_fn=model_fn,\n",
    "#       config=run_config,\n",
    "#       params={\"model_dir\": options['model_dir']},\n",
    "# #       train_batch_size=options['train_batch_size'],\n",
    "# #       eval_batch_size=options['eval_batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:  >>> sup data dir : ./Data/proc_data/GoT/train_20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:looking in ./Data/proc_data/GoT/train_20 for files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:looking in ./Data/proc_data/GoT/train_20 for files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loading training data from these files: ./Data/proc_data/GoT/train_20/tf_examples.tfrecord.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loading training data from these files: ./Data/proc_data/GoT/train_20/tf_examples.tfrecord.0.0\n",
      "INFO:absl:  >>> dev data dir : ./Data/proc_data/GoT/dev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loading eval clas data from these files: ./Data/proc_data/GoT/dev/tf_examples.tfrecord.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loading eval clas data from these files: ./Data/proc_data/GoT/dev/tf_examples.tfrecord.0.0\n",
      "INFO:absl:***** Running training & evaluation *****\n",
      "INFO:absl:  Supervised batch size = 8\n",
      "INFO:absl:  Unsupervised batch size = 0\n",
      "INFO:absl:  Num steps = 3000\n",
      "INFO:absl:  Base evaluation batch size = 8\n",
      "INFO:absl:  Num steps = 312\n",
      "INFO:absl:*** Running training ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:sup batch size: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:sup batch size: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Max Sequence Length = 128 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Max Sequence Length = 128 *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: (), types: tf.string>\n",
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/utils/proc_data_utils.py:118: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/utils/proc_data_utils.py:118: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/utils/proc_data_utils.py:140: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/utils/proc_data_utils.py:140: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:sup batch size: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:sup batch size: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:total sample in a batch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:total sample in a batch: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running train on CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running train on CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/uda.py:250: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/uda.py:250: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n",
      "INFO:absl:Creating supervised model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for IteratorGetNext:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for IteratorGetNext:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/bert/modeling.py:136: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/bert/modeling.py:136: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/bert/modeling.py:136: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/bert/modeling.py:136: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Looking up embeddings using the embedding_lookup funcion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Looking up embeddings using the embedding_lookup funcion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating bert embeddings in the embedding_lookup function of bert/modeling.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating bert embeddings in the embedding_lookup function of bert/modeling.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/bert/modeling.py:408: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/bert/modeling.py:408: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Post-processing the word embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Post-processing the word embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert/embeddings/Reshape_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert/embeddings/Reshape_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding token type embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding token type embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding positional embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding positional embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for IteratorGetNext:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for IteratorGetNext:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for IteratorGetNext:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for IteratorGetNext:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert/embeddings/dropout/mul_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert/embeddings/dropout/mul_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/Reshape_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/Reshape_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/Reshape_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/Reshape_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/bert/modeling.py:678: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/bert/modeling.py:678: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_0/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_0/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_0/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_0/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_1/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_1/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_1/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_1/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_2/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_2/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_2/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_2/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_3/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_3/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_3/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_3/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_4/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_4/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_4/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_4/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_5/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_5/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_5/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_5/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_6/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_6/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_6/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_6/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_7/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_7/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_7/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_7/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_8/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_8/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_8/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_8/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_9/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_9/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_9/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_9/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_10/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_10/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_10/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:asserting rank for bert_1/encoder/layer_10/output/LayerNorm/batchnorm/add_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Adding dropout to layer at rate of 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Normalizing layer - centering and scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/uda.py:317: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/uda.py:317: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/bert/optimization.py:143: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/nlp_finalproject/bert/optimization.py:143: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/base_20/model.ckpt-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/base_20/model.ckpt-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomgoter/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into model/base_20/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into model/base_20/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "# Logical check to determine if we are training (vice evaluating)\n",
    "if options['do_train']:\n",
    "    logging.info(\"  >>> sup data dir : {}\".format(options['sup_train_data_dir']))\n",
    "    \n",
    "    # Are we doing UDA or just simple finetuning?\n",
    "    if options['unsup_ratio'] > 0:\n",
    "        logging.info(\"  >>> unsup data dir : {}\".format(\n",
    "          options['unsup_data_dir']))\n",
    "    \n",
    "    # Pass on all of the training sup/unsup options\n",
    "    train_input_fn = proc_data_utils.training_input_fn_builder(\n",
    "        options['sup_train_data_dir'],\n",
    "        options['unsup_data_dir'],\n",
    "        options['aug_ops'],\n",
    "        options['aug_copy'],\n",
    "        options['unsup_ratio'],\n",
    "        max_seq_len=options['max_seq_length'])\n",
    "\n",
    "# Logical check to see if we are evaluating against the development set (or test set if you change the eval_data_dir)\n",
    "if options['do_eval']:\n",
    "    logging.info(\"  >>> dev data dir : {}\".format(options['eval_data_dir']))\n",
    "    eval_input_fn = proc_data_utils.evaluation_input_fn_builder(\n",
    "        options['eval_data_dir'],\n",
    "        \"clas\")\n",
    "\n",
    "    eval_size = processor.get_dev_size()\n",
    "    eval_steps = int(eval_size / options['eval_batch_size'])\n",
    "\n",
    "# IF we are training and evaluating\n",
    "if options['do_train'] and options['do_eval']:\n",
    "    logging.info(\"***** Running training & evaluation *****\")\n",
    "    logging.info(\"  Supervised batch size = {:d}\".format(\n",
    "        options['train_batch_size']))\n",
    "    logging.info(\"  Unsupervised batch size = {:d}\".format(\n",
    "        options['train_batch_size'] * options['unsup_ratio']))\n",
    "    logging.info(\"  Num steps = {}\".format(options['num_train_steps']))\n",
    "    logging.info(\"  Base evaluation batch size = {:d}\".format(\n",
    "        options['eval_batch_size']))\n",
    "    logging.info(\"  Num steps = {:d}\".format(eval_steps))\n",
    "    \n",
    "    # Initialize\n",
    "    best_acc = 0\n",
    "    \n",
    "    # Looping over training steps by subset (for each checkpoint)\n",
    "    for _ in range(0, options['num_train_steps'], save_checkpoints_steps):\n",
    "        logging.info(\"*** Running training ***\")\n",
    "        \n",
    "        estimator.train(\n",
    "              input_fn=train_input_fn,\n",
    "              steps=save_checkpoints_steps)\n",
    "        \n",
    "        logging.info(\"*** Running evaluation ***\")\n",
    "        dev_result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
    "        logging.info(\">> Results:\")\n",
    "        \n",
    "        # Keep track of the evaluation results\n",
    "        for key in dev_result.keys():\n",
    "            logging.info(\"  {} = {}\".format(key, str(dev_result[key])))\n",
    "            dev_result[key] = dev_result[key].item()\n",
    "        \n",
    "        # Update the best accuracy object\n",
    "        best_acc = max(best_acc, dev_result[\"eval_classify_accuracy\"])\n",
    "    logging.info(\"***** Final evaluation result *****\")\n",
    "    logging.info(\"Best acc: {:.3f}\\n\\n\".format(best_acc))\n",
    "elif options['do_train']:\n",
    "    logging.info(\"***** Running training *****\")\n",
    "    logging.info(\"  Supervised batch size = {}\".format(options['train_batch_size']))\n",
    "    logging.info(\"  Unsupervised batch size = {}\".format(\n",
    "                    options['train_batch_size'] * options['unsup_ratio']))\n",
    "    logging.info(\"  Num steps = {}\".format(options['num_train_steps']))\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=options['num_train_steps'])\n",
    "elif options['do_eval']:\n",
    "    logging.info(\"***** Running evaluation *****\")\n",
    "    logging.info(\"  Base evaluation batch size = {}\".format(options['eval_batch_size']))\n",
    "    logging.info(\"  Num steps = {}\".format(eval_steps))\n",
    "    \n",
    "    # Load in the checkpoint from training to do the evaluation\n",
    "    checkpoint_state = tf.train.get_checkpoint_state(options['model_dir'])\n",
    "\n",
    "    best_acc = 0\n",
    "    for ckpt_path in checkpoint_state.all_model_checkpoint_paths:\n",
    "        if not tf.io.gfile.exists(ckpt_path + \".data-00000-of-00001\"):\n",
    "            logging.info(\n",
    "                \"Warning: checkpoint {:s} does not exist\".format(ckpt_path))\n",
    "        continue\n",
    "        logging.info(\"Evaluating {:s}\".format(ckpt_path))\n",
    "        dev_result = estimator.evaluate(\n",
    "          input_fn=eval_input_fn,\n",
    "          steps=eval_steps,\n",
    "          checkpoint_path=ckpt_path,)\n",
    "        logging.info(\">> Results:\")\n",
    "        \n",
    "        # keep track of evaluation metrics\n",
    "        for key in dev_result.keys():\n",
    "            logging.info(\"  {:s} = {:s}\".format(key, str(dev_result[key])))\n",
    "            dev_result[key] = dev_result[key].item()\n",
    "        \n",
    "        # update our best accuracy variable\n",
    "        best_acc = max(best_acc, dev_result[\"eval_classify_accuracy\"])\n",
    "    logging.info(\"***** Final evaluation result *****\")\n",
    "    logging.info(\"Best acc: {:.3f}\\n\\n\".format(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
