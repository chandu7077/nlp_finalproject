{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W266 Final Project\n",
    "## Chapter Perspective Classification of Game of Thrones Text Passages\n",
    "### Fall 2019\n",
    "### T. P. Goter\n",
    "\n",
    "## Data Cleaning and Text Labeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from glob import glob\n",
    "import spacy\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load in the english spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "line_sep = \"=\"*50 + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataset\n",
    "This portion of the code basically brings in five text files, one for each Game of Thrones book. It loops over each line in each book and assesses whether the line is a chapter title. In Game of Thrones, chapter titles are character names. In each of the text files, chapter titles are presented as the only word on a new line and in all capital letters. Therefore, with some simple string operations we can generate a set of chapter labels. In all there are 17 unique chapter titles, including a \"PROLOGUE\" title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Processing the following files:\n",
      "got_book1\n",
      "got_book2\n",
      "got_book3\n",
      "got_book4\n",
      "got_book5\n",
      "==================================================\n",
      "Processing got_book1\n",
      "HOUSE BARATHEON\n",
      "\n",
      "==================================================\n",
      "Processing got_book2\n",
      "APPENDIX—THE KINGS AND THEIR COURTS\n",
      "\n",
      "==================================================\n",
      "Processing got_book3\n",
      "Appendix\n",
      "\n",
      "==================================================\n",
      "Processing got_book4\n",
      "MEANWHILE, BACK ON THE WALL …\n",
      "\n",
      "==================================================\n",
      "Processing got_book5\n",
      "WESTEROS\n",
      "\n",
      "Generated 20 unique chapter titles\n",
      "The chapter titles are:\n",
      "ALAYNE\n",
      "ARYA\n",
      "BRAN\n",
      "BRIENNE\n",
      "CATELYN\n",
      "CERSEI\n",
      "DAENERYS\n",
      "DAVOS\n",
      "EDDARD\n",
      "EPILOGUE\n",
      "JAIME\n",
      "JON\n",
      "MELISANDRE\n",
      "PROLOGUE\n",
      "REEK\n",
      "SAMWELL\n",
      "SANSA\n",
      "THEON\n",
      "TYRION\n",
      "VICTARION\n"
     ]
    }
   ],
   "source": [
    "# Assumes book text files are in current working directory\n",
    "book_files = sorted([book for book in os.listdir('./') if \"got_book\" in book])\n",
    "\n",
    "print(line_sep + \"Processing the following files:\")\n",
    "for book in book_files:\n",
    "    print(book)\n",
    "\n",
    "# Instantiate some empty objects for use in the loops below\n",
    "titles = set()\n",
    "titles_book = set()\n",
    "books = set()\n",
    "chapter_count = 0\n",
    "dataset = []\n",
    "chapter = \"\"\n",
    "\n",
    "book_ends = ['HOUSE BARATHEON\\n', \n",
    "            'APPENDIX—THE KINGS AND THEIR COURTS\\n',\n",
    "            'Appendix\\n',\n",
    "            'MEANWHILE, BACK ON THE WALL …\\n',\n",
    "            'WESTEROS\\n']\n",
    "\n",
    "# Loop over each book\n",
    "for book in book_files:\n",
    "    flag = False\n",
    "    print(line_sep + \"Processing {}\".format(book))\n",
    "    \n",
    "    # Open the book and read all of the lines in\n",
    "    with open(book, \"r\") as file_in:\n",
    "        book_lines = file_in.readlines()\n",
    "    \n",
    "    # Loop over every line in the current book\n",
    "    for line in book_lines:\n",
    "        \n",
    "        # Don't process anything beyond the last chapter\n",
    "        if line in book_ends:\n",
    "            print(line)\n",
    "            if line == book_ends[-1]:\n",
    "                dataset.append(chapter)\n",
    "            flag = False\n",
    "            break\n",
    "        \n",
    "        # Split the line into words, if applicable\n",
    "        words = line.split()\n",
    "        if len(words) == 1:\n",
    "            \n",
    "            # Chapter titles are just capitalized, multicharacter, names\n",
    "            if words[0].isupper() and words[0].isalpha() and len(words[0])>1:\n",
    "                               \n",
    "                # Maintain a set of chapter titles and a set of chapter titles with the book number\n",
    "                titles_book.add((words[0],book[-1]))\n",
    "                titles.add(words[0])\n",
    "                books.add(book[-1])\n",
    "                \n",
    "                # If we have found a chapter name, update the chapter number\n",
    "                chapter_count += 1\n",
    "                \n",
    "                if flag:\n",
    "                    # Dataset is a list of strings where each string is the entire text from the chapter\n",
    "                    dataset.append((chapter, book[-1]))\n",
    "                \n",
    "                # Reset the chapter string\n",
    "                chapter = \"\"\n",
    "                \n",
    "                # Flag to indicate we are in the text of the book\n",
    "                flag = True\n",
    "        \n",
    "        # Add the current line to the chapter string\n",
    "        if flag:\n",
    "            chapter += line  \n",
    "\n",
    "\n",
    "print(\"Generated {} unique chapter titles\".format(len(titles)))\n",
    "print(\"The chapter titles are:\")\n",
    "for t in sorted(titles):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Chapter Data to Sentence Data\n",
    "In the end we will want to make predictions on short groupings of sentences, not full chapters and not single sentences. But to start lets break the labeled chapters into labeled sentences using the sentencizer from spacy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will add tuples of sentence, label to a new list\n",
    "sentence_ds = []\n",
    "\n",
    "# Create a spacy pipeline that tokenizes and sentencizes our chapters.\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "# nlp.add_pipe(sentencizer)\n",
    "\n",
    "# Create counter for labels\n",
    "d = 0\n",
    "\n",
    "# Loop over all the chapter data\n",
    "for doc in nlp.pipe([chap[0] for chap in dataset if chap[0].split()[0] in titles],\n",
    "                    disable=[\"tagger\", \"parser\", \"ner\", \"textcat\"]):\n",
    "    \n",
    "    # Loop over each sentence\n",
    "    for sent in doc.sents:\n",
    "        words = sent.text.split()\n",
    "        \n",
    "        # Remove chapter label from the first sentence in each chapter\n",
    "        if len(words) > 0 and words[0] in titles:\n",
    "            label = dataset[d][1]\n",
    "            new_sentence = \" \".join(words[1:])\n",
    "        else:\n",
    "            new_sentence = sent.text\n",
    "            \n",
    "        # Add sentence and label to the list\n",
    "        sentence_ds.append((new_sentence, label))\n",
    "    d +=1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Remove extraneous lines from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove dialogue quotes\n",
    "for ds in sentence_ds:   \n",
    "    # Get rid of lines that have Table of Contents in it as these are not part of the text\n",
    "    if 'Table of Contents' in ds[0]:\n",
    "        sentence_ds.remove(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataFrame\n",
    "Let's push our information into a pandas dataframe with column labels. In doing this and exploring the data, it is uncovered that some extraneous page numbers have been treated as sentence beginnings. We remove these. We also remove blank lines. When we are done we reset our index and pickle our sentence/label dataframe for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the clean dataset into a dataframe\n",
    "df = pd.DataFrame(sentence_ds, columns=['text', 'label'])\n",
    "\n",
    "# Place some marker in the text column if it was an empty line\n",
    "df.text = df.text.map(lambda x: 'REMOVE' if len(x.split()) < 1 else x)\n",
    "\n",
    "# Remove the zero-word sentences\n",
    "df = df[~df.text.str.match('^REMOVE')]\n",
    "\n",
    "# Remove leading numbers (likely page numbers introduced from the original copy/paste)\n",
    "df.text = df.text.str.replace('^[0-9]+\\s', '')\n",
    "df.text = df.text.str.replace('[”“]+', '')\n",
    "df.text = df.text.str.replace('[\\n]+', ' ')\n",
    "\n",
    "# Reset dataframe index so rows are sequentially indexed\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Write dataframe to pickle file for later use\n",
    "df.to_pickle('got_sent_w_labels.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Data Exploration\n",
    "We look at how many different sentences we have and their average size. We will combine sentences up to a maximum length of 200 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "There are 151196 total sentences in our dataset.\n",
      "==================================================\n",
      "Median sentence length of 9.0 words\n",
      "==================================================\n",
      "Mean sentence length of 11.3 words\n",
      "==================================================\n",
      "Minimum sentence length of 1 words\n",
      " Wind.\n",
      "==================================================\n",
      "Maximum sentence length of 168 words\n",
      "All of it came pouring out of Brienne then, like black blood from a wound; the betrayals and betrothals, Red Ronnet and his rose, Lord Renly dancing with her, the wager for her maidenhead, the bitter tears she shed the night her king wed Margaery Tyrell, the mêlée at Bitterbridge, the rainbow cloak that she had been so proud of, the shadow in the king’s pavilion, Renly dying in her arms, Riverrun and Lady Catelyn, the voyage down the Trident, dueling Jaime in the woods, the Bloody Mummers, Jaime crying Sapphires, Jaime in the tub at Harrenhal with steam rising from his body, the taste of Vargo Hoat’s blood when she bit down on his ear, the bear pit, Jaime leaping down onto the sand, the long ride to King’s Landing, Sansa Stark, the vow she’d sworn to Jaime, the vow she’d sworn to Lady Catelyn, Oathkeeper, Duskendale, Maidenpool, Nimble Dick and Crackclaw and the Whispers, the men she’d killed … I have to find her, she finished. \n"
     ]
    }
   ],
   "source": [
    "# Add sentence length to dataframe\n",
    "df['sent_length'] = df.text.map(lambda x: len(x.split()))\n",
    "\n",
    "#\n",
    "print(line_sep+\"There are {} total sentences in our dataset.\".format(len(df)))\n",
    "print(line_sep+\"Median sentence length of {} words\".format(df.sent_length.median()))\n",
    "print(line_sep+\"Mean sentence length of {:.1f} words\".format(df.sent_length.mean()))\n",
    "print(line_sep+\"Minimum sentence length of {} words\".format(df.sent_length.min()))\n",
    "print(df.iloc[df.sent_length.idxmin()].text)\n",
    "print(line_sep+\"Maximum sentence length of {} words\".format(df.sent_length.max()))\n",
    "print(df.iloc[df.sent_length.idxmax()].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151196"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = list(df.text)\n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0 \n",
    "c = -1\n",
    "grouped_sent = \"\"\n",
    "groups = []\n",
    "while sents:\n",
    "    while x < 80: \n",
    "        if (c > 0 ) and len(grouped_sent) != 0 and ((df.loc[c-1].label != df.loc[c].label) or not sents):\n",
    "            break\n",
    "        grouped_sent += \" \" + sents.pop(0)\n",
    "        x = len(grouped_sent.split())\n",
    "        c += 1\n",
    "    groups.append((grouped_sent, df.loc[c].label))\n",
    "    grouped_sent = \"\"\n",
    "    x=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "There are 19438 total sequences in our dataset.\n",
      "==================================================\n",
      "Median sequence length of 86.0 words\n",
      "==================================================\n",
      "Mean sequence length of 88.1 words\n",
      "==================================================\n",
      "Minimum sequence length of 54 words\n",
      " yet now were heard again, ringing from the timbers of her father’s hall: The King in the North!  The King in the North!  THE KING IN THE NORTH! The comet’s tail spread across the dawn, a red slash that bled above the crags of Dragonstone like a wound in the pink and purple sky.\n",
      "==================================================\n",
      "Maximum sequence length of 209 words\n",
      " Galladon drowned when I was four and he was eight, though, and Alysanne and Arianne died still in the cradle. I am the only child the gods let him keep. The freakish one, not fit to be a son or daughter. All of it came pouring out of Brienne then, like black blood from a wound; the betrayals and betrothals, Red Ronnet and his rose, Lord Renly dancing with her, the wager for her maidenhead, the bitter tears she shed the night her king wed Margaery Tyrell, the mêlée at Bitterbridge, the rainbow cloak that she had been so proud of, the shadow in the king’s pavilion, Renly dying in her arms, Riverrun and Lady Catelyn, the voyage down the Trident, dueling Jaime in the woods, the Bloody Mummers, Jaime crying Sapphires, Jaime in the tub at Harrenhal with steam rising from his body, the taste of Vargo Hoat’s blood when she bit down on his ear, the bear pit, Jaime leaping down onto the sand, the long ride to King’s Landing, Sansa Stark, the vow she’d sworn to Jaime, the vow she’d sworn to Lady Catelyn, Oathkeeper, Duskendale, Maidenpool, Nimble Dick and Crackclaw and the Whispers, the men she’d killed … I have to find her, she finished. \n"
     ]
    }
   ],
   "source": [
    "group_df = pd.DataFrame(groups,columns=['text', 'label'])\n",
    "\n",
    "# Add sequence length to dataframe\n",
    "group_df['seq_length'] = group_df.text.map(lambda x: len(x.split()))\n",
    "\n",
    "#\n",
    "print(line_sep+\"There are {} total sequences in our dataset.\".format(len(group_df)))\n",
    "print(line_sep+\"Median sequence length of {} words\".format(group_df.seq_length.median()))\n",
    "print(line_sep+\"Mean sequence length of {:.1f} words\".format(group_df.seq_length.mean()))\n",
    "print(line_sep+\"Minimum sequence length of {} words\".format(group_df.seq_length.min()))\n",
    "print(group_df.iloc[group_df.seq_length.idxmin()].text)\n",
    "print(line_sep+\"Maximum sequence length of {} words\".format(group_df.seq_length.max()))\n",
    "print(group_df.iloc[group_df.seq_length.idxmax()].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>seq_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We should start back, Gared urged as the wood...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will saw them, Gared said.  If he says they ...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>His voice echoed, too loud in the twilit fore...</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yet it was more than that. Under the wounded ...</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Until tonight. Something was different tonig...</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  seq_length\n",
       "0   We should start back, Gared urged as the wood...     1          80\n",
       "1    Will saw them, Gared said.  If he says they ...     1          80\n",
       "2   His voice echoed, too loud in the twilit fore...     1          96\n",
       "3   Yet it was more than that. Under the wounded ...     1         101\n",
       "4    Until tonight. Something was different tonig...     1          84"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the Data\n",
    "Resample our dataframe to mix up the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>seq_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12299</th>\n",
       "      <td>Was my father murdered?  So your mother beli...</td>\n",
       "      <td>4</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13802</th>\n",
       "      <td>A blasphemous name. We prefer to call him Dri...</td>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4809</th>\n",
       "      <td>if you think your Sea Bitch can keep up with ...</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>You think I should mistrust Lannister because...</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17745</th>\n",
       "      <td>The candle burned with a dark red flame, she ...</td>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text label  seq_length\n",
       "12299    Was my father murdered?  So your mother beli...     4          88\n",
       "13802   A blasphemous name. We prefer to call him Dri...     4          81\n",
       "4809    if you think your Sea Bitch can keep up with ...     2          90\n",
       "470     You think I should mistrust Lannister because...     1          82\n",
       "17745   The candle burned with a dark red flame, she ...     5          80"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_df = group_df.sample(frac=1.0)\n",
    "group_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df = group_df.drop('seq_length',axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate into test, development, and training data\n",
    "\n",
    "There are 19,438 different pieces of data. We will use 15000 as training data, 2500 as development data and the rest as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    4635\n",
       "3    4575\n",
       "2    3608\n",
       "1    3313\n",
       "4    3307\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = group_df.loc[:15000]\n",
    "dev_df = group_df.loc[15000:17500]\n",
    "test_df = group_df.loc[17500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle('train/train.pkl')\n",
    "dev_df.to_pickle('dev/dev.pkl')\n",
    "test_df.to_pickle('test/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['4', '2', '1', '5', '3'], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SplitData.ipynb         got_book3               realdonaldtrump.ndjson\r\n",
      "\u001b[34mdev\u001b[m\u001b[m/                    got_book4               \u001b[34mtest\u001b[m\u001b[m/\r\n",
      "generate_data.ipynb     got_book5               \u001b[34mtrain\u001b[m\u001b[m/\r\n",
      "got_book1               got_sent_w_labels.pkl\r\n",
      "got_book2               \u001b[34mproc_data\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mData\u001b[m\u001b[m/             \u001b[34mbert_pretrained\u001b[m\u001b[m/  \u001b[34muda\u001b[m\u001b[m/              \u001b[34mxlnet_pretrained\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-aaeb166dec04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
