{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "## Game of Thrones Text Classification\n",
    "### T. P. Goter\n",
    "### W266 Final Project\n",
    "### Fall 2019\n",
    "\n",
    "This notebook is used to generate a Naive Bayes model for text classification using training data generated for the Game of Thrones novel. A simple GridSearch with cross validation is done for each model in order to empirically determine the best parameter of for smoothing (i.e., alpha). Both unigram and bigram models are considered. Additionally, consideration is given to a tfidf vectorication vice a simple word count vectorization. The results of these studies show that all four models perform about the same and have precision, recall and f1 scores near 0.70. Class 5 (i.e., Book 5) shows as the most easily predicted. This isn't really surprising given books four and five were focused on a subset of the characters. Thus, word counts of character names likely are better indicators for these books. With 70% accuracy as our baseline score on the development set, there is plenty of room for further improvement with BERT and the UDA techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# import the tokenization module\n",
    "from utils import tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into dataframes stored in a dictionary\n",
    "dfs = {}\n",
    "for data in 'train dev test'.split():\n",
    "    dfs[data] = pd.read_pickle('Data/' + data + '/' + data + '.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_model(df_train, df_dev, bigram=False, tfidf=False, tokenize=False):\n",
    "    '''\n",
    "    Function to train and evalute a multinomial naive bayes model.\n",
    "    :param: bigram: Boolean, use unigram and bigrams\n",
    "    :param: tfidf: Boolean, use tfidf weighting during feature vectorization\n",
    "    :param: tokenize: Boolean, use WordPiece tokenization with BERT input vocabulary\n",
    "    '''\n",
    "    # Set up a range of alphas to test\n",
    "    alphas = {'alpha': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "     \n",
    "    # Get data and labels from dataframe\n",
    "    if tokenize: \n",
    "        train_data = df_train.tokens.map(lambda x: \" \".join(x))\n",
    "        dev_data = df_dev.tokens.map(lambda x: \" \".join(x))\n",
    "    else:\n",
    "        train_data = df_train.text\n",
    "        dev_data = df_dev.text\n",
    "    \n",
    "    train_y = df_train.label\n",
    "    dev_y = df_dev.label\n",
    "    \n",
    "    # Instantiate the count vectorizer\n",
    "    if bigram:\n",
    "        if tfidf:\n",
    "            vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "        else:\n",
    "            vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "    else:\n",
    "        if tfidf:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "        else:\n",
    "            vectorizer = CountVectorizer()\n",
    "    \n",
    "    # Generate the Feature Vectors\n",
    "    train_X = vectorizer.fit_transform(train_data)    \n",
    "    \n",
    "    # Generate the vocabulary for the dev data\n",
    "    dev_X = vectorizer.transform(dev_data)\n",
    "       \n",
    "    # Fit the model\n",
    "    clf = MultinomialNB()\n",
    "    \n",
    "    # Run a grid search over alpha (smoothing) values to determine best \n",
    "    gs_clf = GridSearchCV(clf, param_grid=alphas, cv=5, return_train_score=True)\n",
    "    gs_clf.fit(train_X, train_y)\n",
    "    \n",
    "    # Display the best parameter\n",
    "    print(50 * \"=\")\n",
    "    print(\"The best alpha value was determined to be {}\".format(gs_clf.best_params_['alpha']))\n",
    "    print(50 * \"=\")\n",
    "\n",
    "    # Let's make some predictions using the best classifier\n",
    "    y_pred = gs_clf.best_estimator_.predict(dev_X)\n",
    "       \n",
    "    print(classification_report(y_pred, dev_y))\n",
    "    print(confusion_matrix(dev_y, y_pred))\n",
    "    \n",
    "    return gs_clf.best_estimator_.feature_log_prob_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Unigram Model\n",
    "\n",
    "Metrics we will consider are precision, recall, and F1-score. Remember that:\n",
    "\n",
    "- Precision: Number of items labeled as class A that are actually class A. - How many false positives?\n",
    "- Recall: Number of items labeled as class A normalized to all things that are class A. - How many false negatives?\n",
    "- F1 Score: Harmonic mean of Precision and Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "The best alpha value was determined to be 0.5\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.73      0.76       427\n",
      "           2       0.61      0.70      0.65       400\n",
      "           3       0.64      0.62      0.63       608\n",
      "           4       0.79      0.73      0.75       484\n",
      "           5       0.76      0.79      0.77       582\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      2501\n",
      "   macro avg       0.72      0.71      0.71      2501\n",
      "weighted avg       0.72      0.71      0.71      2501\n",
      "\n",
      "[[311  35  39   4   7]\n",
      " [ 45 280  85  29  20]\n",
      " [ 43  56 380  50  66]\n",
      " [ 10  17  37 351  31]\n",
      " [ 18  12  67  50 458]]\n"
     ]
    }
   ],
   "source": [
    "uni_probs = create_basic_model(dfs['train'], dfs['dev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "The best alpha value was determined to be 0.1\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.75      0.75       395\n",
      "           2       0.57      0.67      0.61       390\n",
      "           3       0.68      0.62      0.65       650\n",
      "           4       0.74      0.75      0.74       439\n",
      "           5       0.80      0.77      0.78       627\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      2501\n",
      "   macro avg       0.70      0.71      0.71      2501\n",
      "weighted avg       0.71      0.71      0.71      2501\n",
      "\n",
      "[[296  39  45   8   8]\n",
      " [ 53 260  95  25  26]\n",
      " [ 29  55 403  36  72]\n",
      " [  7  23  49 328  39]\n",
      " [ 10  13  58  42 482]]\n"
     ]
    }
   ],
   "source": [
    "bi_probs = create_basic_model(dfs['train'], dfs['dev'], bigram=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Model with Tf-Idf Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "The best alpha value was determined to be 0.1\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.70      0.77      0.73       360\n",
      "           2       0.54      0.71      0.61       346\n",
      "           3       0.74      0.56      0.64       790\n",
      "           4       0.69      0.80      0.74       384\n",
      "           5       0.78      0.76      0.77       621\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      2501\n",
      "   macro avg       0.69      0.72      0.70      2501\n",
      "weighted avg       0.71      0.70      0.69      2501\n",
      "\n",
      "[[276  36  73   2   9]\n",
      " [ 35 246 129  21  28]\n",
      " [ 27  36 440  25  67]\n",
      " [  9  18  66 307  46]\n",
      " [ 13  10  82  29 471]]\n"
     ]
    }
   ],
   "source": [
    "uni_tfidf_probs = create_basic_model(dfs['train'], dfs['dev'], tfidf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Model with Tf-Idf Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "The best alpha value was determined to be 0.01\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.76      0.73       369\n",
      "           2       0.53      0.64      0.58       377\n",
      "           3       0.69      0.57      0.63       714\n",
      "           4       0.69      0.77      0.73       395\n",
      "           5       0.79      0.74      0.76       646\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      2501\n",
      "   macro avg       0.68      0.70      0.69      2501\n",
      "weighted avg       0.69      0.69      0.69      2501\n",
      "\n",
      "[[281  47  54   2  12]\n",
      " [ 46 241 122  22  28]\n",
      " [ 26  54 410  28  77]\n",
      " [  6  19  63 306  52]\n",
      " [ 10  16  65  37 477]]\n"
     ]
    }
   ],
   "source": [
    "bi_tfidf_probs = create_basic_model(dfs['train'], dfs['dev'], tfidf=True, bigram=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
